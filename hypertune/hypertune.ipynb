{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from pathlib import Path\n",
    "from ray import tune\n",
    "import time\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from mads_datasets.base import BaseDatastreamer\n",
    "from mltrainer.preprocessors import BasePreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainstreamer = BaseDatastreamer(traindataset, preprocessor = BasePreprocessor(), batchsize=config.batchsize)\n",
    "teststreamer = BaseDatastreamer(testdataset, preprocessor = BasePreprocessor(), batchsize=config.batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 5\n",
    "N_EXPERIMENTS = 18\n",
    "tune_dir = Path(\"models/ray/\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperOptSearch()\n",
    "scheduler = AsyncHyperBandScheduler(\n",
    "    time_attr=\"training_iteration\", grace_period=1, reduction_factor=3, max_t=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = {}\n",
    "best_config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"input_size\": 3,\n",
    "    \"output_size\": 20,\n",
    "    \"dropout\": 0.1,\n",
    "    \"epochs\": MAX_EPOCHS,\n",
    "    \"hidden_size\": tune.randint(16, 512),\n",
    "    \"num_layers\": tune.randint(1, 8),\n",
    "    \"tune_dir\": tune_dir,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(config):\n",
    "    \"\"\"\n",
    "    Function to tune a model using Ray Tune.\n",
    "    Args:\n",
    "        config (dict): Hyperparameter configuration passed by Ray Tune.\n",
    "    \"\"\"\n",
    "    import mlflow\n",
    "    from mltrainer import Trainer, TrainerSettings, ReportTypes\n",
    "    from dataclasses import asdict\n",
    "    import torch\n",
    "    from metrics import accuracy, f1micro, f1macro, precision, recall  # Ensure these metrics are imported\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam\n",
    "\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run():\n",
    "        settings = TrainerSettings(\n",
    "            epochs=5,  # Or adjust based on experiment\n",
    "            metrics=[accuracy, f1micro, f1macro, precision, recall],\n",
    "            logdir=\"logs/heart2D\",  # Adjust as necessary\n",
    "            train_steps=len(trainstreamer),  # Ensure trainstreamer is globally accessible\n",
    "            valid_steps=len(teststreamer),  # Ensure teststreamer is globally accessible\n",
    "            reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "            scheduler_kwargs=None,\n",
    "            earlystop_kwargs=None\n",
    "        )\n",
    "\n",
    "        # Set tags and log parameters\n",
    "        mlflow.set_tag(\"model\", \"CNN\")\n",
    "        mlflow.set_tag(\"dataset\", \"heart2D\")\n",
    "        mlflow.set_tag(\"dev\", dev)  # Ensure `dev` is defined\n",
    "        mlflow.log_param(\"scheduler\", \"None\")\n",
    "        mlflow.log_param(\"earlystop\", \"None\")\n",
    "        mlflow.log_params(asdict(config))\n",
    "        mlflow.log_param(\"epochs\", settings.epochs)\n",
    "        mlflow.log_param(\"matrix0\", config[\"matrixshape\"][0])\n",
    "        mlflow.log_param(\"matrix1\", config[\"matrixshape\"][1])\n",
    "        mlflow.log_param(\"optimizer\", str(optimizer))\n",
    "        mlflow.log_params(settings.optimizer_kwargs)\n",
    "\n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,  # Ensure model is globally accessible or passed in config\n",
    "            settings=settings,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            traindataloader=trainstreamer.stream(),\n",
    "            validdataloader=teststreamer.stream(),\n",
    "            scheduler=None,\n",
    "            device=device,  # Ensure device is defined\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        trainer.loop()\n",
    "\n",
    "        # Calculate metrics\n",
    "        cfm = caluclate_cfm(model, teststreamer)  # Ensure caluclate_cfm function is available\n",
    "        for i, tp in enumerate(np.diag(cfm)):\n",
    "            mlflow.log_metric(f\"TP_{i}\", tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tune_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      2\u001b[0m analysis \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtune_model\u001b[49m,\n\u001b[1;32m      4\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m      5\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     local_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(tune_dir),\n\u001b[1;32m      8\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39mN_EXPERIMENTS,\n\u001b[1;32m      9\u001b[0m     stop\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m: MAX_EPOCHS},\n\u001b[1;32m     10\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m     11\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m timer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mray_hyperband\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tic\n\u001b[1;32m     16\u001b[0m best \u001b[38;5;241m=\u001b[39m analysis\u001b[38;5;241m.\u001b[39mget_best_config()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tune_model' is not defined"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "analysis = tune.run(\n",
    "    tune_model,\n",
    "    config=config,\n",
    "    metric=\"valid_loss\",\n",
    "    mode=\"min\",\n",
    "    local_dir=str(tune_dir),\n",
    "    num_samples=N_EXPERIMENTS,\n",
    "    stop={\"training_iteration\": MAX_EPOCHS},\n",
    "    scheduler=scheduler,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "timer[\"ray_hyperband\"] = time.time() - tic\n",
    "\n",
    "best = analysis.get_best_config()\n",
    "best[\"accuracy\"] = analysis.best_result[\"accuracy\"]\n",
    "best_config[\"hyperband\"] = best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
